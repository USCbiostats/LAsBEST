---
title:  "<span style=color:#003399> Machine Learning II </span>"
author: "Juan Pablo Lewinger"
date: "07/21/2023"
output: ioslides_presentation
widescreen: true
smaller: true
keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## <span style="color:#660066"> Last time </span>{.smaller}
- **Linear regression - least squares**

- **A linear model fit in R:**
    + ``lm(Brain.weight ~ Head.size + Sex + Age, data = brain_train)``
   
- **Training and evaluating performance on same data overestimates performance**
    + Randomly split into training and test sets (e.g. 80% training, 20% test): 
      + Train model on training set
      + *Estimate* prediction performance on test set
    + Performance metrics: MSE, RMSE (absolute) and $R^2$ (relative)

- **For predicting on new data, use best possible model: i.e. model trained on all available data**

- **For prediction with LR, linearity and equal variance assumptions are not required (but if assumptions hold model may predict better)**  

## <span style="color:#660066"> Interactions and higher order terms</span>

A model with an interaction using the interaction ``:`` operator:

```{r, include=FALSE}
options(digits=3)
setwd("/Users/jp/Google Drive/Teaching/Machine Learning/2022/Lectures/Lecture 2 - Linear regression 1")
brain_data = read.table("brain.txt", header=T)
n = nrow(brain_data)
brain_data$Sex = factor(brain_data$Sex, levels=1:2, labels=c("Male", "Female"))
brain_data$Age = factor(brain_data$Age, levels=1:2, labels=c("20-46", "46+"))

set.seed(303)
trainset = sample(1:n, floor(0.7*n))
brain_train = brain_data[trainset,]; n_train = nrow(brain_train)
brain_test = brain_data[-trainset,]; n_test = nrow(brain_test)
```

```{r}
brain_lm1 = lm(Brain.weight ~  Sex + Head.size + Head.size:Sex , data = brain_train)
summary(brain_lm1)$coefficients
```

## <span style="color:#660066"> Interactions and higher order terms</span>
Same model can be fitted using the ``*`` operator:
```{r}
brain_lm1 = lm(Brain.weight ~  Sex*Head.size, data = brain_train)
summary(brain_lm1)$coefficients
```

## <span style="color:#660066"> Interactions and higher order terms</span>
We can also add non-linear terms:

```{r}
brain_lm2 = lm(Brain.weight ~ Head.size + I(Head.size^2), data = brain_train)
summary(brain_lm2)$coefficients
```

## <span style="color:#660066"> Model selection </span> 
- So far we've dealt with fitting a single model
   + Train on training data
   + Assess prediction performance on test data
- In practice we want to choose from many possible models
  + Include/exclude particular features?
  + Include interactions or higher order terms?
  + Competing algorithms (e.g. linear regression vs. KNN)
- **How do we choose?**

## <span style="color:#660066"> Model selection -- example </span>{.smaller}
  
Let's fit models of increasing complexity on the brain data:

```{r, include=FALSE}
rmse = function(observed, predicted) sqrt(mean((observed-predicted)^2))
```
```{r}
brain_fit1 = lm(Brain.weight ~ Head.size, data=brain_train)

brain_fit2 = lm(Brain.weight ~ Head.size + Sex + Age, data=brain_train)

brain_fit3 = lm(Brain.weight ~ (Head.size + Sex + Age)^2, data=brain_train)

brain_fit4 = lm(Brain.weight ~ (Head.size + I(Head.size^2) + Sex + Age)^2, data=brain_train)
```

## <span style="color:#660066"> Model selection -- example </span>{.smaller}

Let's look at their **training** RSMEs:

```{r, include=FALSE}
rmse_train1 = rmse(brain_train$Brain.weight, fitted(brain_fit1))
rmse_train2 = rmse(brain_train$Brain.weight, fitted(brain_fit2))
rmse_train3 = rmse(brain_train$Brain.weight, fitted(brain_fit3))
rmse_train4 = rmse(brain_train$Brain.weight, fitted(brain_fit4))
train.error = c(rmse_train1, rmse_train2, rmse_train3, rmse_train4)
```

```{r, echo=FALSE, fig.height = 4, fig.width = 6}
par(mar=c(5,5,1,1))
plot(1:4, train.error, pch=16, col='red4', xlab='Model', ylab='RMSE', cex=3, cex.lab=2, cex.axis=2)
lines(1:4, train.error, col='gray70', lwd=4)
```

Which model should we choose?

## <span style="color:#660066"> Model selection -- example </span>{.smaller} 

Let's look at their **validation** RSMEs:

```{r, include=FALSE}
rmse_test1 = rmse(brain_test$Brain.weight, predict(brain_fit1, brain_test))
rmse_test2 = rmse(brain_test$Brain.weight, predict(brain_fit2, brain_test))
rmse_test3 = rmse(brain_test$Brain.weight, predict(brain_fit3, brain_test))
rmse_test4 = rmse(brain_test$Brain.weight, predict(brain_fit4, brain_test))
test.error = c(rmse_test1, rmse_test2, rmse_test3, rmse_test4)
```

```{r, echo=FALSE, fig.height = 4, fig.width = 6}
par(mar=c(5,5,1,1))
plot(1:4, test.error, pch=16, col='steelblue', xlab='Model', ylab='RMSE', cex=3, cex.lab=2, cex.axis=2)
lines(1:4, test.error, col='gray70', lwd=4)
```

Which model should we choose?

## <span style="color:#660066"> Model selection -- example </span>{.smaller}  

Let's look at their **train and validation** RSMEs:

```{r, echo=FALSE, fig.height = 4, fig.width = 6}
par(mar=c(5,5,1,1))
plot(1:4, test.error, pch=16, col='steelblue', xlab='Model', ylab='RMSE', cex=3, cex.lab=2, cex.axis=2, ylim=range(c(test.error, train.error)))
lines(1:4, test.error, col='gray70', lwd=4)
points(1:4, train.error, pch=16, col='red4', cex=3)
lines(1:4, train.error, col='gray70', lwd=4)

```

Which model should we choose?


## <span style="color:#660066"> Model selection -- example </span> 

```{r, echo=FALSE, fig.height = 4, fig.width = 6}
par(mar=c(5,5,1,1))
plot(1:4, test.error, pch=16, col='steelblue', xlab='Model', ylab='RMSE', cex=3, cex.lab=2, cex.axis=2, ylim=range(c(test.error, train.error)))
lines(1:4, test.error, col='gray70', lwd=4)
points(1:4, train.error, pch=16, col='red4', cex=3)
lines(1:4, train.error, col='gray70', lwd=4)
```

Pick Model 2, ``Brain.weight ~ Head.size + Sex + Age``, which has the best test/validation error among the 4 models we considered 


## <span style="color:#660066"> Model selection </span> 
- Picking model with smallest training MSE or highest training $R^2$ is a bad idea.

- Right thing to do is split data into training and testing as before and choose model with smallest **test** MSE or highest **test** $R^2$.

- This is a fair way to compare prediction performance across different models. 

- BUT, how do we get and estimate of prediction performance for the best model? 

- Already 'used up' test set for choosing best model

## <span style="color:#660066"> Model selection -- Validation set </span> 

- Choosing smallest **test** MSE or highest **test** $R^2$ will give overly optimistic assessment of prediction error

- Reason is analogous to training error overestimating prediction performance
    + E.g. we fit linear regression by minimizing residual sum of squares RSS
    + In training data model fits better than in test data

- Minimizing test error over several models makes the best model 'look too good'  

- Need a separate, never-used test set for final estimate of prediction performance!

## <span style="color:#660066"> Model selection -- Validation set </span> 

- For model selection we split the data in 3: **training, validation, and test sets**:

    + **Training set** we use to fit all models considered (e.g. several linear regression models)

    + **Validation set** we use to perform model selection -- pick model that gives best prediction error on validation set

    + **Test set** we **only use once** to get an unbiased final estimate of the prediction error for the selected model

- Only works if we have relatively large $n$ (we'll assume so for now) 

- For smaller $n$ we can repeat the split into training and validation sets multiple time (cross-validation)
  
## <span style="color:#660066"> Model selection -- simulation </span>{.smaller}
```{r, echo=FALSE}

set.seed(203)
n_train = 40
noise = 5
x = seq(1, 5, length.out = n_train); y = exp(x)*cos(x) + rnorm(n_train, mean=0, sd=noise)
train = data.frame(x = x, y = y)
                  
n_valid = 40
x = seq(1, 5, length.out = n_valid); y = exp(x)*cos(x) + rnorm(n_valid, mean=0, sd=noise)
validation = data.frame(x = x, y = y)
```
- Consider some simulated training data: $\;n=40$ 
- Generating model/ true relationship is non-linear: $\; y = e^x \cos{(x)} + \epsilon$
- $\epsilon \sim N(0, \sigma^2)$, $\;\sigma=5$


```{r, echo=FALSE, fig.height = 4, fig.width = 6}
par(mar=c(5,5,1,1))
plot(train$x, train$y, xlab='x', ylab='y', pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2)
curve(exp(x)*cos(x), lwd=3, col='gray70', add=T)
```

## <span style="color:#660066"> Model selection -- simulation </span> 

We'll fit polynomial regression models of increasing degree/complexity:

\[
\begin{eqnarray}
Y & = & \beta_0 & + & \beta_1 X + \epsilon \\
Y & = & \beta_0 & + & \beta_1 X +  \beta_1 X^2 +\epsilon \\
\vdots & \\
Y & = & \beta_0 & + & \beta_1 X + \beta_1 X^2  + ... + \beta_{25} X^{25} + \epsilon\\
\end{eqnarray}
\]

## <span style="color:#660066"> Model selection -- simulation </span> {.smaller}

```{r, echo=T}
fit1 = lm(y ~ x, data = train)
fit2 = lm(y ~ x + I(x^2), data = train)
fit3 = lm(y ~ x + I(x^2) + I(x^3), data = train)
fit4 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = train)
fit5 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5), data = train)
fit6 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = train)
fit7 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7), data = train)
fit8 = lm(y ~ poly(x, 8), data = train)  
fit9 = lm(y ~ poly(x, 9), data = train)
fit15 = lm(y ~ poly(x, 15), data = train)
fit20 = lm(y ~ poly(x, 20), data = train)
fit25 = lm(y ~ poly(x, 25), data = train)
```

Using poly() is more compact and numerically better!

## <span style="color:#660066"> Model selection -- simulation </span> 

```{r, echo=FALSE, fig.height = 4.5, fig.width = 10}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))
plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 1', cex.main=2)
lines(train$x, fitted(fit1), lwd=5, col='orange')

plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 2', cex.main=2)
lines(train$x, fitted(fit2), lwd=5, col='orange')
```

## <span style="color:#660066"> Model selection -- simulation </span> 

```{r, echo=FALSE, fig.height = 4.5, fig.width = 10}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))
plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 3', cex.main=2)
lines(train$x, fitted(fit3), lwd=5, col='orange')

plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 4', cex.main=2)
lines(train$x, fitted(fit4), lwd=5, col='orange')
```

## <span style="color:#660066"> Model selection -- simulation </span>{.smaller}

```{r, echo=FALSE, fig.height = 4.5, fig.width = 10}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))
plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 5', cex.main=2)
lines(train$x, fitted(fit5), lwd=5, col='orange')

plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 6', cex.main=2)
lines(train$x, fitted(fit6), lwd=5, col='orange')
```

## <span style="color:#660066"> Model selection -- simulation </span>{.smaller} 

```{r, echo=FALSE, fig.height = 4.5, fig.width = 10}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))
plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 7', cex.main=2)
lines(train$x, fitted(fit7), lwd=5, col='orange')

plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 8', cex.main=2)
lines(train$x, fitted(fit8), lwd=5, col='orange')
```

## <span style="color:#660066"> Model selection -- simulation </span>{.smaller}

```{r, echo=FALSE, fig.height = 4.5, fig.width = 10}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))
plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 20', cex.main=2)
lines(train$x, fitted(fit20), lwd=5, col='orange')

plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 25', cex.main=2)
lines(train$x, fitted(fit25), lwd=5, col='orange')
```

## <span style="color:#660066"> Model selection -- simulation </span>{.smaller} 

Training and validation RMSEs:

```{r, echo=FALSE, fig.height = 4, fig.width = 7}
rmse_train1 = rmse(train$y, fitted(fit1))
rmse_train2 = rmse(train$y, fitted(fit2))
rmse_train3 = rmse(train$y, fitted(fit3))
rmse_train4 = rmse(train$y, fitted(fit4))
rmse_train5 = rmse(train$y, fitted(fit5))
rmse_train6 = rmse(train$y, fitted(fit6))
rmse_train7 = rmse(train$y, fitted(fit7))
rmse_train8 = rmse(train$y, fitted(fit8))
rmse_train9 = rmse(train$y, fitted(fit9))
rmse_train15 = rmse(train$y, fitted(fit15))
rmse_train20 = rmse(train$y, fitted(fit20))
rmse_train25 = rmse(train$y, fitted(fit25))
train.error = c('p=1'=rmse_train1, 'p=2'=rmse_train2, 'p=3'=rmse_train3, 'p=4'=rmse_train4, 'p=5'=rmse_train5, 'p=6'=rmse_train6, 'p=7'=rmse_train7, 'p=8'=rmse_train8, 'p=9'=rmse_train9,'p=15'=rmse_train15, 'p=20'=rmse_train20, 'p=25'=rmse_train25)

rmse_val1 = rmse(validation$y, predict(fit1, validation))
rmse_val2 = rmse(validation$y, predict(fit2, validation))
rmse_val3 = rmse(validation$y, predict(fit3, validation))
rmse_val4 = rmse(validation$y, predict(fit4, validation))
rmse_val5 = rmse(validation$y, predict(fit5, validation))
rmse_val6 = rmse(validation$y, predict(fit6, validation))
rmse_val7 = rmse(validation$y, predict(fit7, validation))
rmse_val8 = rmse(validation$y, predict(fit8, validation))
rmse_val9 = rmse(validation$y, predict(fit9, validation))
rmse_val15 = rmse(validation$y, predict(fit15, validation))
rmse_val20 = rmse(validation$y, predict(fit20, validation))
rmse_val25 = rmse(validation$y, predict(fit25, validation))

val.error = c('p=1'=rmse_val1, 'p=2'=rmse_val2, 'p=3'=rmse_val3, 'p=4'=rmse_val4, 'p=5'=rmse_val5, 'p=6'=rmse_val5, 'p=7'=rmse_val7, 'p=8'=rmse_val8, 'p=9'=rmse_val9,'p=15'=rmse_val15, 'p=20'=rmse_val20, 'p=25'=rmse_val25)

round(rbind('Training' = train.error, 'Validation' = val.error), 2)

par(mar=c(5,5,1,1))
degree = c(1:9, 15, 20, 25)
plot(degree, val.error, ylab='Error (RMSE)', pch=16, cex=3, col='red4', cex.lab=2, cex.axis=2, ylim=range(c(val.error, train.error)))
lines(degree, val.error, lwd=4, col='red4')
points(degree, train.error, pch=16, cex=3, col='steelblue')
lines(degree, train.error, lwd=4, col='steelblue')
```

## <span style="color:#660066"> Model selection -- simulation </span>{.smaller}

Training and validation RMSEs:
```{r, echo=FALSE}
round(rbind('Training' = train.error, 'Validation' = val.error), 2)
```
- Model with $p = 4$ is 'just right': it **minimizes the validation error**
- (But the RMSE is estimated with error in the validation set due to finite sample size)
- (Could also perhaps choose $p = 5$ or $p=6$ since RMSE is quite close to that of model with $p=4$)

## <span style="color:#660066"> Model selection - summary </span>{.smaller}
- We typically want try several models because we don't know beforehand which one will predict best 

- Cannot compare models based on the training error because more complex models will always have smaller training error

- Should use a separate validation set to compare models

- But validation error does not give honest assessment of prediction error:
    + Need yet a separate test set to evaluate performance
    
## <span style="color:#660066"> Overfitting and Underfitting </span>{.smaller} 
- Overfitting occurs when the model captures the noise rather than the trend in the training data:
    + The model fits the training data 'too well' 
    + Model closely follows the quirks of the training data but does not generalize/predict well 
    + Fails to capture the true underlying relationship between features and outcome
- Underfitting occurs when the model can't capture the true underlying relationship between features and outcome 
- The more complex/flexible the model the higher the potential for overfitting
- Need to choose the right level of complexity to avoid overfitting
- One way to accomplish this is by performing model selection, i.e. comparing different models using a validation set

## <span style="color:#660066"> Overfitting </span>{.smaller} 
```{r, echo=FALSE, fig.height = 4.5, fig.width = 10}

set.seed(203)
n_train = 40
noise = 5
x = seq(1, 5, length.out = n_train); y = exp(x)*cos(x) + rnorm(n_train, mean=0, sd=noise)
train = data.frame(x = x, y = y)
                  
n_valid = 40
x = seq(1, 5, length.out = n_valid); y = exp(x)*cos(x) + rnorm(n_valid, mean=0, sd=noise)
validation = data.frame(x = x, y = y)

fit1 = lm(y ~ x, data = train)
fit2 = lm(y ~ x + I(x^2), data = train)
fit3 = lm(y ~ x + I(x^2) + I(x^3), data = train)
fit4 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = train)
fit5 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5), data = train)
fit6 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = train)
fit7 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7), data = train)
fit8 = lm(y ~ poly(x, 8), data = train)
fit9 = lm(y ~ poly(x, 9), data = train)
fit15 = lm(y ~ poly(x, 15), data = train)
fit20 = lm(y ~ poly(x, 20), data = train)
fit25 = lm(y ~ poly(x, 25), data = train)

rmse = function(observed, predicted) sqrt(mean((observed-predicted)^2))

rmse_train1 = rmse(train$y, fitted(fit1))
rmse_train2 = rmse(train$y, fitted(fit2))
rmse_train3 = rmse(train$y, fitted(fit3))
rmse_train4 = rmse(train$y, fitted(fit4))
rmse_train5 = rmse(train$y, fitted(fit5))
rmse_train6 = rmse(train$y, fitted(fit6))
rmse_train7 = rmse(train$y, fitted(fit7))
rmse_train8 = rmse(train$y, fitted(fit8))
rmse_train9 = rmse(train$y, fitted(fit9))
rmse_train15 = rmse(train$y, fitted(fit15))
rmse_train20 = rmse(train$y, fitted(fit20))
rmse_train25 = rmse(train$y, fitted(fit25))

rmse_val1 = rmse(validation$y, predict(fit1, validation))
rmse_val2 = rmse(validation$y, predict(fit2, validation))
rmse_val3 = rmse(validation$y, predict(fit3, validation))
rmse_val4 = rmse(validation$y, predict(fit4, validation))
rmse_val5 = rmse(validation$y, predict(fit5, validation))
rmse_val6 = rmse(validation$y, predict(fit6, validation))
rmse_val7 = rmse(validation$y, predict(fit7, validation))
rmse_val8 = rmse(validation$y, predict(fit8, validation))
rmse_val9 = rmse(validation$y, predict(fit9, validation))
rmse_val15 = rmse(validation$y, predict(fit15, validation))
rmse_val20 = rmse(validation$y, predict(fit20, validation))
rmse_val25 = rmse(validation$y, predict(fit25, validation))

par(mfrow = c(1,2))
par(mar=c(5,5,3,3))

plot(train$x, train$y, pch=16, cex=1.5, col='red4', cex.lab=2, cex.axis=2, main='Training - p = 4 and p = 25', cex.main=2, ylim=range(c(train$y, validation$y)))
lines(train$x, fitted(fit4), lwd=3, col='#00B075')
lines(train$x, fitted(fit25), lwd=3, col='orange')

plot(validation$x, validation$y, pch=16, cex=1.5, col='steelblue4', cex.lab=2, cex.axis=2, main='Validation - p = 4 and p = 25', cex.main=2, ylim=range(c(train$y, validation$y)))
lines(train$x, fitted(fit4), lwd=3, col='#00B075')
lines(validation$x, fitted(fit25), lwd=3, col='orange')
```

- **Overfitting** -- Model with $p=25$ fitted the noise in the training data 
- In validation set polynomial of order $p=25$ performs very poorly 

## <span style="color:#660066"> Uderfitting </span> {.smaller}

```{r, echo=FALSE, fig.height = 4.5, fig.width = 10}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))

plot(train$x, train$y, pch=16, cex=1.5, col='red4', cex.lab=2, cex.axis=2, main='Training - p = 2 and p = 4', cex.main=2, ylim=range(c(train$y, validation$y)))
lines(train$x, fitted(fit4), lwd=3, col='#00B075')
lines(train$x, fitted(fit2), lwd=3, col='orange')

plot(validation$x, validation$y, pch=16, cex=1.5, col='steelblue4', cex.lab=2, cex.axis=2, main='Validation - p = 2 and p = 4', cex.main=2, ylim=range(c(train$y, validation$y)))
lines(train$x, fitted(fit4), lwd=3, col='#00B075')
lines(validation$x, fitted(fit2), lwd=3, col='orange')
```
- **Underfitting** -- Model with $p=2$ not flexible enough to capture underlying trend
- In validation set  polynomial of order $p=2$  performs poorly 

## <span style="color:#660066"> Overfitting and Underfitting </span> {.smaller}

**Preventing overfitting and underfitting is a central issue in Machine Learning**

- Prevent underfitting by considering sufficiently flexible models

- Prevent overfitting by avoiding overly complex models 

- In practice we don't know beforehand the right level of complexity

- We choose model complexity/perform model selection using validation set

- Model **complexity/flexibility/capacity** increases with:
    + Number of features
    + Degree of polynomial terms 
    + Number of interaction terms
    + Type of model (e.g. nonparametric KNN is much more flexible than linear regression)
    

## <span style="color:#660066"> Formal ML setting </span> {.smaller}

- Training set: $\;(\mathbf{X}_1, Y_1),(\mathbf{X}_2, Y_2),...,(\mathbf{X}_n, Y_n)$

- Vector of features for $i^{th}$ sample: $\;\mathbf{X}_i = (X_{i1},...,X_{ip})$

- **Only assumption is independence across samples (instances/observations/subjects)**

- $Y = f(X_1,...,X_p) + \epsilon$, 

- $f(X_1,...,X_p) = E[Y|X_1,...,X_p]$

- $\epsilon = Y - f(X_1,...,X_p)$

- $E[\epsilon] = E[Y-E[Y|X_1,...,X_p]] = E[Y]- E[E[Y|X_1,...,X_p]] = 0$

## <span style="color:#660066"> Formal ML setting </span> {.smaller} 

- In supervised ML goal is to estimate regression function $f$ to make predictions about $Y$
- In statistics goal is to make inference:
    + which predictor $X_1,...,X_p$ is associated with $Y$? (e.g. test for association)
    + what is the relationship between each predictor and the response? (e.g. estimate effect size)
- In Biomedical problems interest is usually in both prediction and inference
- In ML we emphasize prediction. In statistics we emphasize inference.
- In general, need more assumptions to make inferences than prediction
    + data follows particular distribution e.g. normal, binomial, multinomial, poisson
    
## <span style="color:#660066">Reducible and irreducible errors in prediction </span> {.smaller}

- Assume we have and estimate of the regression function $f$, $\widehat{f}$ (e.g. from fitting a linear regression)

- The average or expected test mean square error is given by:

\[
\begin{eqnarray}
E[MSE] & = & E[(Y - \widehat{Y})^2] & = & \\
    & = & E[(f(\mathbf{X}) + \epsilon - \widehat{f}(\mathbf{X}))^2] & = &\\
    & = & E[(f(\mathbf{X}) - \widehat{f}(\mathbf{X}))^2]  + Var[\epsilon] & &
\end{eqnarray}
\]

- $Var[\epsilon]$ is the 'irreducible error':
    + unknown, intrinsic noise to the problem -- can't do anything about it.
- $E[(f(\mathbf{X}) - \widehat{f}(\mathbf{X}))^2]$ is the reducible error:
    + We can make it smaller by using a better estimate $\widehat{f}$ of $\;f$
    + E.g. add higher order terms or use more flexible/complex model (e.g. KNN)
    + Goal is to estimate $f$ to make the reducible error as small as possible
    
## <span style="color:#660066"> Bias--Variance Trade-off </span> {.smaller}
 
- The reducible error can in turn be partitioned as:

\[
\begin{eqnarray}
& & E \left[ \left( f(\mathbf{X}) - \widehat{f}(\mathbf{X}) \right)^2 \right]  =  \\ 
 & = &  E \left[ \left (f(\mathbf{X}) - E[\widehat{f}](\mathbf{X}) \right)^2 \right]  +  E \left[ \left( \widehat{f}(\mathbf{X}) - E[\widehat{f}](\mathbf{X}) \right)^2 \right] & = & \\
 & = & \textrm{Bias} \left( \widehat{f}(\mathbf{X}) \right)^2  +  \textrm{Var} \left( \widehat{f}(\mathbf{X}) \right) & &
\end{eqnarray}
\]

## <span style="color:#660066"> Bias--Variance Trade-off </span> 
 
- Average prediction error  = bias^2 + variance

- **Bias** term represents how far the average estimated regression function is from the true regression function across training sets

- **Variance** term represents how variable the estimated regression function is across training sets 

## <span style="color:#660066"> Bias--Variance Trade-off </span> 

- Consider our simulated example:
    + True relationship is: $\; Y = e^X \cos{(X)} + \epsilon$, $\; \epsilon \sim N(0, \sigma^2)$
- We'll compare a simple vs. complex linear regression fitted on training data generated under model above:
    + Simple: $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$
    + Complex: $Y = \beta_0 + \beta_1 X + ... + \beta_{25} X^{25} + \epsilon$

## <span style="color:#660066"> Bias--Variance Trade-off </span> {.smaller}

```{r, echo=FALSE, fig.height = 4.5, fig.width = 10}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))

plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 2', cex.main=2)
lines(train$x, fitted(fit2), lwd=5, col='orange')

plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 25', cex.main=2)
lines(train$x, fitted(fit25), lwd=5, col='orange')
```

Which fitted curve will change more if the training set changes?

## <span style="color:#660066"> Bias--Variance Trade-off </span> {.smaller}

Two different training sets 

```{r, echo=FALSE, fig.height = 4.5, fig.width = 10}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))

y2 = exp(x)*cos(x) + rnorm(n_train, mean=0, sd=noise)
train2 = data.frame(x = x, y = y2)
fit2_2 = lm(y ~ poly(x, 2), data = train2)
fit25_2 = lm(y ~ poly(x, 25), data = train2)

plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 2', cex.main=2, ylim=c(-45,45))
lines(train$x, fitted(fit2), lwd=5, col='orange')

plot(train2$x, train2$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 2', cex.main=2, ylim=c(-45,45))
lines(train2$x, fitted(fit2_2), lwd=5, col='orange')
```

## <span style="color:#660066"> Bias--Variance Trade-off </span> {.smaller}
 
Two different training sets 

```{r, echo=FALSE, fig.height = 4.5, fig.width = 10}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))

plot(train$x, train$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 25', cex.main=2, ylim=c(-45,45))
lines(train$x, fitted(fit25), lwd=5, col='orange')

plot(train2$x, train2$y, pch=16, cex=2, col='red4', cex.lab=2, cex.axis=2, main='p = 25', cex.main=2, ylim=c(-45,45))
lines(train2$x, fitted(fit25_2), lwd=5, col='orange')
```

## <span style="color:#660066"> Bias--Variance Trade-off </span> {.smaller}

10 different training sets

```{r, echo=FALSE}
reps = 10
fitted = replicate(reps, {
  y = exp(x)*cos(x) + rnorm(n_train, mean=0, sd=noise)
  train = data.frame(x = x, y = y)
  fit2 = lm(y ~ poly(x, 2), data = train)
  fit25 = lm(y ~ poly(x, 25), data = train)
  cbind(fitted(fit2), fitted(fit25))
})
```

```{r, echo=FALSE, fig.height = 4, fig.width = 9}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))

plot(train$x, train$y, type='n', cex.lab=2, cex.axis=2, main='p = 2', cex.main=2, ylim=c(-45,45))
for(i in 1:reps) lines(train$x, fitted[, 1, i], lwd=5, col='orange')

plot(train$x, train$y, type='n', cex.lab=2, cex.axis=2, main='p = 25', cex.main=2, ylim=c(-45,45))
for(i in 1:reps) lines(train$x, fitted[, 2, i], lwd=5, col='orange')
```

- Simple model $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$ exhibits **low variance** 
- Complex model $Y = \beta_0 + \beta_1 X + ... + \beta_{25} X^{25} + \epsilon$ exhibits **high variance**

## <span style="color:#660066"> Bias--Variance Trade-off </span> {.smaller}

Let's add the true regression function: $\;f(x)=exp(x)cos(x)$

```{r, echo=FALSE, fig.height = 4, fig.width = 9}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))

curve(exp(x)*cos(x), from=1, to=5, lwd=4, col='gray70', cex.lab=1.5, cex.axis=2, main='p = 2', cex.main=2, ylim=c(-45,45), ylab='True regression function')
for(i in 1:reps) lines(train$x, fitted[, 1, i], lwd=5, col='orange')

curve(exp(x)*cos(x), from=1, to=5, lwd=4, col='gray70', cex.lab=1.5, cex.axis=2, main='p = 25', cex.main=2, ylim=c(-45,45), ylab='True regression function')
for(i in 1:reps) lines(train$x, fitted[, 2, i], lwd=5, col='orange')
curve(exp(x)*cos(x), from=1, to=5, lwd=4, col='gray70', add=T)
```

Which model approximates the true regression line more closely?

## <span style="color:#660066"> Bias--Variance Trade-off </span> {.smaller}

Average regression line over 10 different training sets

```{r, echo=FALSE, fig.height = 4, fig.width = 9}
par(mfrow = c(1,2))
par(mar=c(5,5,3,3))

curve(exp(x)*cos(x), from=1, to=5, lwd=4, col='gray70', cex.lab=1.5, cex.axis=2, main='p = 2', cex.main=2, ylim=c(-45,45), ylab='True regression function')
lines(train$x, apply(fitted[, 1, ], 1, mean), lwd=5, col='red2')


curve(exp(x)*cos(x), from=1, to=5, lwd=4, col='gray70', cex.lab=1.5, cex.axis=2, main='p = 25', cex.main=2, ylim=c(-45,45), ylab='True regression function')
lines(train$x, apply(fitted[, 2, ], 1, mean), lwd=5, col='red2')
```

- Simple model $Y = \beta_0 + \beta_1 X + \epsilon$ exhibits **high bias**
- Complex model $Y = \beta_0 + \beta_1 X + ... + \beta_{25} X^{25} + \epsilon$ exhibits **low bias**


## <span style="color:#660066"> Bias--Variance Trade-off </span> {.smaller}

- Goal of model selection in regression is to minimize reducible error

- Reducible error = bias^2 + variance

- Complex/flexible models exhibit high variance but low bias

- Simple/rigid models exhibit low variance but high bias

- In model selection we find a model with 'just the right' amount of complexity:
    + model complexity with the best bias and variance trade-off
    + that minimizes reducible error
    
